# ğŸ•·ï¸ JW.org Meeting Content API

API automatizada para contenido semanal de reuniones de Testigos de JehovÃ¡ mediante scraping de JW.org.

## ğŸ¯ **PropÃ³sito**

Este proyecto genera automÃ¡ticamente una API REST con datos estructurados de las reuniones semanales, incluyendo:

- **Reuniones entre semana** (Vida y Ministerio Cristianos)
- **Reuniones de fin de semana** (ReuniÃ³n PÃºblica + Estudio de La Atalaya)
- **Materiales multimedia** (videos, imÃ¡genes, audio)
- **InformaciÃ³n de cÃ¡nticos**
- **DuraciÃ³n y estructura de secciones**

## ğŸ—ï¸ **Arquitectura**

```
ğŸ“ jw-scraping/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ scrape-weekly.yml     # GitHub Actions para scraping automÃ¡tico
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.js           # Scraper principal con Puppeteer
â”‚   â”œâ”€â”€ validator.js         # ValidaciÃ³n de datos
â”‚   â”œâ”€â”€ api-generator.js     # Generador de endpoints API
â”‚   â””â”€â”€ summary.js           # Generador de resÃºmenes
â”œâ”€â”€ data/
â”‚   â””â”€â”€ {year}/
â”‚       â””â”€â”€ week-{nn}.json   # Datos semanales
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ index.json          # InformaciÃ³n general de la API
â”‚   â”œâ”€â”€ latest.json         # Semana mÃ¡s reciente
â”‚   â”œâ”€â”€ weeks.json          # Lista de semanas
â”‚   â”œâ”€â”€ stats.json          # EstadÃ­sticas generales
â”‚   â””â”€â”€ data/               # Copia de todos los datos
â””â”€â”€ package.json
```

## ğŸš€ **Uso Local**

### InstalaciÃ³n
```bash
npm install
```

### Scraping manual
```bash
# Scrapear semana actual
npm run scrape

# Scrapear semana especÃ­fica
npm run scrape -- --week=2025-01-20

# Forzar actualizaciÃ³n
npm run scrape -- --force

# Diferente idioma
npm run scrape -- --language=en
```

### ValidaciÃ³n
```bash
npm run validate
```

### Generar API
```bash
npm run generate-api
```

### Resumen
```bash
npm run summary
```

## ğŸ¤– **AutomatizaciÃ³n con GitHub Actions**

### ConfiguraciÃ³n
1. **Habilitar GitHub Pages** en tu repositorio
2. **Configurar fuente** como GitHub Actions
3. El workflow se ejecuta **cada lunes a las 6:00 AM UTC**

### EjecuciÃ³n manual
1. Ve a **Actions** en tu repositorio
2. Selecciona **"Scrape Weekly Meeting Content"**
3. Haz clic en **"Run workflow"**
4. Configura parÃ¡metros opcionales

### URLs de la API
Una vez configurado, la API estarÃ¡ disponible en:

```
https://TU-USUARIO.github.io/jw-scraping/
```

## ğŸ“¡ **Endpoints de la API**

### ğŸ“‹ **InformaciÃ³n general**
```
GET /index.json
```
Metadatos de la API, endpoints disponibles y estadÃ­sticas generales.

### ğŸ• **Ãšltima semana**
```
GET /latest.json
```
Datos completos de la semana mÃ¡s reciente.

### ğŸ“… **Lista de semanas**
```
GET /weeks.json
```
Resumen de todas las semanas disponibles con enlaces a datos completos.

### ğŸ“Š **EstadÃ­sticas**
```
GET /stats.json
```
EstadÃ­sticas detalladas: total de reuniones, materiales, duraciÃ³n, etc.

### ğŸ—‚ï¸ **Datos especÃ­ficos**
```
GET /data/{year}/week-{number}.json
```

Ejemplos:
- `/data/2025/week-03.json` - Tercera semana de 2025
- `/data/2025/week-15.json` - Decimoquinta semana de 2025

## ğŸ“„ **Estructura de Datos**

### Formato de semana completa
```json
{
  "id": "week-2025-3",
  "weekStartDate": "2025-01-13T00:00:00.000Z",
  "weekEndDate": "2025-01-19T00:00:00.000Z",
  "weekOf": "Semana del 13-19 ene",
  "year": 2025,
  "weekNumber": 3,
  "meetings": [
    {
      "type": "midweek",
      "title": "ReuniÃ³n Entre Semana",
      "date": "2025-01-15T00:00:00.000Z",
      "sections": [
        {
          "id": "section-1",
          "order": 1,
          "title": "CÃ¡ntico y oraciÃ³n",
          "duration": 5,
          "type": "song",
          "materials": []
        }
      ],
      "materials": {
        "videos": [],
        "images": [],
        "audio": [],
        "songs": []
      },
      "totalDuration": 105
    }
  ],
  "stats": {
    "totalSections": 12,
    "totalMaterials": 8,
    "totalDuration": 225
  },
  "metadata": {
    "generatedAt": "2025-01-20T06:30:00.000Z",
    "version": "1.0",
    "language": "es"
  }
}
```

## ğŸ”§ **ConfiguraciÃ³n**

### Variables de entorno
```bash
# Opcional: configurar directorio de salida
CONTENT_OUTPUT_DIR=./custom-data
API_OUTPUT_DIR=./custom-api

# Opcional: configurar idioma
SCRAPER_LANGUAGE=es
```

### PersonalizaciÃ³n del scraper
Edita `src/scraper.js` para:
- Agregar nuevos selectores CSS
- Modificar lÃ³gica de extracciÃ³n
- Agregar soporte para mÃ¡s idiomas
- Cambiar formato de salida

## ğŸ› ï¸ **Desarrollo**

### Ejecutar en modo desarrollo
```bash
npm run dev
```

### Testing
```bash
npm test
```

### Servidor de preview local
```bash
npm run preview
```

## ğŸ“Š **Monitoreo**

### Logs del GitHub Actions
- Ve a la pestaÃ±a **Actions** de tu repositorio
- Cada ejecuciÃ³n muestra logs detallados
- Los errores se reportan automÃ¡ticamente

### ValidaciÃ³n automÃ¡tica
- Todos los datos se validan antes de generar la API
- Los archivos con errores no se publican
- Los errores se muestran en el resumen del workflow

## ğŸ¤ **Contribuir**

1. **Fork** este repositorio
2. **Crea** una rama para tu feature
3. **Haz** tus cambios
4. **AsegÃºrate** de que los tests pasen
5. **EnvÃ­a** un Pull Request

### Reportar problemas
- Usa los **Issues** para reportar bugs
- Incluye logs del scraper si es posible
- Especifica la semana que causÃ³ problemas

## âš–ï¸ **Consideraciones Legales**

- Este proyecto hace scraping **responsable** de contenido pÃºblico
- **Respeta** los tÃ©rminos de uso de JW.org
- **No sobrecarga** los servidores (mÃ¡ximo 1 ejecuciÃ³n semanal)
- Los datos extraÃ­dos son para **uso personal/religioso**

## ğŸ“œ **Licencia**

MIT License - ver archivo `LICENSE` para detalles.

## ğŸ†˜ **Soporte**

Â¿Problemas? Â¡Estamos aquÃ­ para ayudar!

1. **Revisa** la documentaciÃ³n arriba
2. **Busca** en Issues existentes
3. **Crea** un nuevo Issue con detalles
4. **Incluye** logs y configuraciÃ³n relevante

---

**â­ Si este proyecto te es Ãºtil, Â¡dale una estrella en GitHub!**